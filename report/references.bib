@article{ghiasiPlugInInversionModelAgnostic2021,
  title = {Plug-{{In Inversion}}: {{Model-Agnostic Inversion}} for {{Vision}} with {{Data Augmentations}}},
  shorttitle = {Plug-{{In Inversion}}},
  author = {Ghiasi, Amin and Kazemi, Hamid and Reich, Steven and Zhu, Chen and Goldblum, Micah and Goldstein, Tom},
  date = {2021-10-06},
  url = {https://openreview.net/forum?id=RVdN1-eDZ1b},
  urldate = {2025-08-26},
  abstract = {Existing techniques for model inversion typically rely on hard-to-tune regularizers, such as total variation or feature regularization, which must be individually calibrated for each network in order to produce adequate images. In this work, we introduce Plug-In Inversion, which relies on a simple set of augmentations and does not require excessive hyper-parameter tuning. Under our proposed augmentation-based scheme, the same set of augmentation hyper-parameters can be used for inverting a wide range of image classification models, regardless of input dimensions or the architecture. We illustrate the practicality of our approach by inverting Vision Transformers (ViTs) and Multi-Layer Perceptrons (MLPs) trained on the ImageNet dataset, tasks which to the best of our knowledge have not been successfully accomplished by any previous works.},
  langid = {english}
}

@online{kazemiWhatWeLearn2024,
  title = {What Do We Learn from Inverting {{CLIP}} Models?},
  author = {Kazemi, Hamid and Chegini, Atoosa and Geiping, Jonas and Feizi, Soheil and Goldstein, Tom},
  date = {2024-03-05},
  eprint = {2403.02580},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.02580},
  url = {http://arxiv.org/abs/2403.02580},
  urldate = {2024-12-16},
  abstract = {We employ an inversion-based approach to examine CLIP models. Our examination reveals that inverting CLIP models results in the generation of images that exhibit semantic alignment with the specified target prompts. We leverage these inverted images to gain insights into various aspects of CLIP models, such as their ability to blend concepts and inclusion of gender biases. We notably observe instances of NSFW (Not Safe For Work) images during model inversion. This phenomenon occurs even for semantically innocuous prompts, like “a beautiful landscape,” as well as for prompts involving the names of celebrities.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@inproceedings{mahendranUnderstandingDeepImage2015,
  title = {Understanding {{Deep Image Representations}} by {{Inverting Them}}},
  author = {Mahendran, Aravindh and Vedaldi, Andrea},
  date = {2015},
  pages = {5188--5196},
  url = {https://openaccess.thecvf.com/content_cvpr_2015/html/Mahendran_Understanding_Deep_Image_2015_CVPR_paper.html},
  urldate = {2025-08-26},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}
}

@inproceedings{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date = {2021-07-01},
  pages = {8748--8763},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/radford21a.html},
  urldate = {2025-08-30},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@inproceedings{shenRANSACFlowGenericTwoStage2020,
  title = {{{RANSAC-Flow}}: {{Generic Two-Stage Image Alignment}}},
  shorttitle = {{{RANSAC-Flow}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Shen, Xi and Darmon, François and Efros, Alexei A. and Aubry, Mathieu},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  pages = {618--637},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-58548-8_36},
  abstract = {This paper considers the generic problem of dense alignment between two images, whether they be two frames of a video, two widely different views of a scene, two paintings depicting similar content, etc. Whereas each such task is typically addressed with a domain-specific solution, we show that a simple unsupervised approach performs surprisingly well across a range of tasks. Our main insight is that parametric and non-parametric alignment methods have complementary strengths. We propose a two-stage process: first, a feature-based parametric coarse alignment using one or more homographies, followed by non-parametric fine pixel-wise alignment. Coarse alignment is performed using RANSAC on off-the-shelf deep features. Fine alignment is learned in an unsupervised way by a deep network which optimizes a standard structural similarity metric (SSIM) between the two images, plus cycle-consistency. Despite its simplicity, our method shows competitive results on a range of tasks and datasets, including unsupervised optical flow on KITTI, dense correspondences on Hpatches, two-view geometry estimation on YFCC100M, localization on Aachen Day-Night, and, for the first time, fine alignment of artworks on the Brughel dataset. Our code and data are available at http://imagine.enpc.fr/\textasciitilde shenx/RANSAC-Flow/.},
  isbn = {978-3-030-58548-8},
  langid = {english},
  keywords = {Applications to art,Unsupervised dense image alignment}
}

@inproceedings{yinDreamingDistillDataFree2020,
  title = {Dreaming to {{Distill}}: {{Data-Free Knowledge Transfer}} via {{DeepInversion}}},
  shorttitle = {Dreaming to {{Distill}}},
  author = {Yin, Hongxu and Molchanov, Pavlo and Alvarez, Jose M. and Li, Zhizhong and Mallya, Arun and Hoiem, Derek and Jha, Niraj K. and Kautz, Jan},
  date = {2020},
  pages = {8715--8724},
  url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Yin_Dreaming_to_Distill_Data-Free_Knowledge_Transfer_via_DeepInversion_CVPR_2020_paper.html},
  urldate = {2025-08-26},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}
}

@inproceedings{yinSeeGradientsImage2021,
  title = {See through {{Gradients}}: {{Image Batch Recovery}} via {{GradInversion}}},
  shorttitle = {See through {{Gradients}}},
  author = {Yin, Hongxu and Mallya, Arun and Vahdat, Arash and Alvarez, Jose M. and Kautz, Jan and Molchanov, Pavlo},
  date = {2021-06},
  pages = {16332--16341},
  publisher = {IEEE},
  location = {Nashville, TN, USA},
  doi = {10.1109/CVPR46437.2021.01607},
  url = {https://ieeexplore.ieee.org/document/9577731/},
  urldate = {2025-06-29},
  abstract = {Training deep neural networks requires gradient estimation from data batches to update parameters. Gradients per parameter are averaged over a set of data and this has been presumed to be safe for privacy-preserving training in joint, collaborative, and federated learning applications. Prior work only showed the possibility of recovering input data given gradients under very restrictive conditions – a single input point, or a network with no non-linearities, or a small 32 ˆ 32 px input batch. Therefore, averaging gradients over larger batches was thought to be safe. In this work, we introduce GradInversion, using which input images from a larger batch (8 – 48 images) can also be recovered for large networks such as ResNets (50 layers), on complex datasets such as ImageNet (1000 classes, 224 ˆ 224 px). We formulate an optimization task that converts random noise into natural images, matching gradients while regularizing image fidelity. We also propose an algorithm for target class label recovery given gradients. We further propose a group consistency regularization framework, where multiple agents starting from different random seeds work together to find an enhanced reconstruction of the original data batch. We show that gradients encode a surprisingly large amount of information, such that all the individual images can be recovered with high fidelity via GradInversion, even for complex datasets, deep networks, and large batch sizes.},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-6654-4509-2},
  langid = {english}
}

@inproceedings{zhangUnreasonableEffectivenessDeep2018,
  title = {The {{Unreasonable Effectiveness}} of {{Deep Features}} as a {{Perceptual Metric}}},
  author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
  date = {2018},
  pages = {586--595},
  url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_The_Unreasonable_Effectiveness_CVPR_2018_paper.html},
  urldate = {2025-07-28},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}
}

@article{zhouwangImageQualityAssessment2004,
  title = {Image Quality Assessment: From Error Visibility to Structural Similarity},
  shorttitle = {Image Quality Assessment},
  author = {{Zhou Wang} and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  date = {2004-04},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume = {13},
  number = {4},
  pages = {600--612},
  issn = {1057-7149, 1941-0042},
  doi = {10.1109/TIP.2003.819861},
  url = {https://ieeexplore.ieee.org/document/1284395/},
  urldate = {2025-09-04},
  langid = {english}
}
